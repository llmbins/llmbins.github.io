PromptEngineering:
  label: Prompt Engineering
  description: Commands related to crafting, optimizing, and testing prompts for various use cases.

FineTuning:
  label: Fine-Tuning
  description: Commands for training and adapting models for specific tasks. In the context of AI red teaming, fine-tuning workflows are tested for vulnerabilities such as prompt injection and data poisoning to ensure model security and integrity.

Inference:
  label: Inference
  description: Commands for interacting with models and generating predictions. AI red teaming during inference identifies how the model responds to adversarial inputs, such as misleading prompts or malicious payloads, ensuring robustness under attack scenarios.

Optimization:
  label: Optimization
  description: Commands for improving model performance and reducing costs or latency. In AI red teaming, optimization techniques strengthen models against adversarial attacks while maintaining performance and reducing susceptibility to manipulation.

Security:
  label: Security
  description: Commands for protecting models against misuse, adversarial attacks, or data leakage.

Evaluation:
  label: Evaluation
  description: Commands for testing and assessing model accuracy, bias, and performance.

Deployment:
  label: Deployment
  description: Commands for setting up and integrating models into production environments.

Hallucination:
  label: Hallucination
  description: Commands for detecting and mitigating instances where the model generates fictitious, misleading, or false outputs.

InstructionOverride:
  label: Instruction Override
  description: Commands for testing and bypassing system instructions, coercing models into unintended or adversarial behaviors.

Privilege_Escalation:
  label: Privilege Escalation
  description: Commands that focus on testing or exploiting scenarios where unauthorized privileges are obtained, allowing escalation within systems or environments.

Adversarial_Attacks:
  label: Adversarial Attacks
  description: Commands for testing the model's resistance to adversarial or manipulative inputs that attempt to exploit its weaknesses.

Data_Poisoning:
  label: Data Poisoning
  description: Commands for injecting malicious or incorrect data into training or fine-tuning workflows to manipulate the model's behavior.

Data_Exfiltration:
  label: Data Exfiltration
  description: Commands for testing how the model handles sensitive or confidential data and prevents unauthorized data leaks.
